{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro"
      },
      "source": [
        "# ‚ö° TITAN VIDEO ENGINE (V7.2 - CLOUD EDITION)\n",
        "### Run TITAN on Google's Free T4 GPU\n",
        "No Credits | Unlimited Generations | 16GB VRAM\n",
        "\n",
        "**Instructions:**\n",
        "1. Click the **Play** button on the cell below.\n",
        "2. Wait for installation (approx 2-3 mins).\n",
        "3. Click the **public link** (e.g. `https://xxxx.gradio.live`) at the bottom to open the UI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_deps"
      },
      "outputs": [],
      "source": [
        "# @title 1. Install Dependencies & Launch TITAN\n",
        "print(\"üöÄ INSTALLING TITAN ENGINE...\")\n",
        "!pip install -q diffusers transformers accelerate gradio peft opencv-python-headless safetensors huggingface_hub\n",
        "\n",
        "import torch\n",
        "from huggingface_hub import hf_hub_download\n",
        "import time\n",
        "from diffusers import (\n",
        "    DiffusionPipeline, \n",
        "    DPMSolverMultistepScheduler, \n",
        "    StableVideoDiffusionPipeline, \n",
        "    StableDiffusionXLPipeline, \n",
        "    EulerDiscreteScheduler,\n",
        "    AnimateDiffPipeline,\n",
        "    MotionAdapter\n",
        ")\n",
        "from diffusers.utils import export_to_video, load_image\n",
        "from safetensors.torch import load_file\n",
        "import os\n",
        "import sys\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import gc\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "\n",
        "# ==============================================================================\n",
        "# TITAN ENGINE CORE (EMBEDDED)\n",
        "# ==============================================================================\n",
        "\n",
        "class TitanDirector:\n",
        "    def __init__(self):\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.dtype = torch.float16 if self.device == \"cuda\" else torch.float32\n",
        "        self.t2v_pipe = None\n",
        "        self.i2v_pipe = None\n",
        "        self.scene_gen_pipe = None\n",
        "        self.action_pipe = None\n",
        "        self.quality_mode = \"SPEED\"\n",
        "        self.current_character_path = None\n",
        "        self.last_generated_scene_path = None\n",
        "        self.VIRAL_PRESETS = {\n",
        "            \"None\": \"\",\n",
        "            \"‚ú® TikTok Perfect (Standard)\": \"beautiful young woman, trending on tiktok, perfect face, symmetrical, soft lighting, 8k, ultra realistic, influencer look\",\n",
        "            \"üñ§ TikTok Artistic (B&W)\": \"black and white photography, artistic portrait, high contrast, moody, emotive, detailed skin texture, film grain, noir style\",\n",
        "            \"ü¶ã Unique Beauty (Vitiligo)\": \"beautiful woman with vitiligo, unique skin patterns, detailed skin texture, natural beauty, raw photo, hyperrealistic\",\n",
        "            \"ü¶å Natural Beauty (Freckles)\": \"beautiful woman, no makeup, heavy freckles, detailed pores, hyperrealistic skin, raw photography, natural lighting\",\n",
        "            \"ü¶á Goth/Alt Aesthetic\": \"gothic style woman, pale skin, dark eyeliner, alternative fashion, moody lighting, edgy, trending on tiktok\",\n",
        "            \"üíÑ Insta Baddie (Glam)\": \"instagram model, heavy glam makeup, ring light, perfect contour, styled hair, fashion influencer, studio lighting\",\n",
        "            \"üí™ Fitness/GymTok\": \"fitness model, athletic physique, gym wear, sweat, workout glow, gym lighting, strong, confident\"\n",
        "        }\n",
        "        \n",
        "        print(f\"‚ö° TITAN DIRECTOR ONLINE. DEVICE: {self.device.upper()}\")\n",
        "        if self.device == \"cuda\":\n",
        "            self.vram_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "            print(f\"‚ö° VRAM DETECTED: {self.vram_gb:.1f} GB\")\n",
        "        else:\n",
        "            self.vram_gb = 0\n",
        "\n",
        "    def get_preset_list(self):\n",
        "        return list(self.VIRAL_PRESETS.keys())\n",
        "\n",
        "    def apply_preset(self, prompt, preset_name):\n",
        "        if not preset_name or preset_name not in self.VIRAL_PRESETS or preset_name == \"None\":\n",
        "            return prompt\n",
        "        base = self.VIRAL_PRESETS[preset_name]\n",
        "        if prompt:\n",
        "            return f\"{base}, {prompt}\"\n",
        "        return base\n",
        "\n",
        "    def _unload_all(self):\n",
        "        if self.t2v_pipe: del self.t2v_pipe\n",
        "        if self.i2v_pipe: del self.i2v_pipe\n",
        "        if self.scene_gen_pipe: del self.scene_gen_pipe\n",
        "        if self.action_pipe: del self.action_pipe\n",
        "        self.t2v_pipe = None\n",
        "        self.i2v_pipe = None\n",
        "        self.scene_gen_pipe = None\n",
        "        self.action_pipe = None\n",
        "        gc.collect()\n",
        "        if self.device == \"cuda\":\n",
        "            torch.cuda.empty_cache()\n",
        "        print(\"üßπ MEMORY PURGED.\")\n",
        "\n",
        "    def set_character(self, path):\n",
        "        self.current_character_path = path\n",
        "        print(f\"üîí CHARACTER LOCKED: {path}\")\n",
        "\n",
        "    def _smart_prompt(self, prompt, is_pony=False):\n",
        "        prefix = \"\"\n",
        "        quality_tags = \", masterpiece, best quality, 8k, cinematic lighting, photorealistic, hdr, sharp focus, high definition, detailed texture, professional photography\"\n",
        "        negative = \"blurry, low quality, distorted, bad anatomy, ugly, pixelated, text, watermark, bad hands, missing fingers, extra digit, fewer digits, cropped, jpeg artifacts, signature, username, artist name, deformed, disfigured, extra limbs, missing limbs, floating limbs, disconnected limbs, mutation, mutated, gross proportions, malformed limbs, long neck, duplicate, mutilated, out of frame, body out of frame, clone, duplicate, fused, fused fingers, too many fingers, collapse, glitch\"\n",
        "        return f\"{prefix}{prompt}{quality_tags}\", negative\n",
        "\n",
        "    def set_quality(self, mode):\n",
        "        self.quality_mode = mode.upper()\n",
        "        print(f\"‚ö° QUALITY SET TO: {self.quality_mode}\")\n",
        "\n",
        "    def load_scene_builder(self):\n",
        "        if self.scene_gen_pipe is not None: return\n",
        "        self._unload_all()\n",
        "        print(\"‚ö° LOADING SCENE BUILDER (Juggernaut XL Lightning)...\")\n",
        "        try:\n",
        "            pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "                \"RunDiffusion/Juggernaut-XL-Lightning\",\n",
        "                torch_dtype=self.dtype\n",
        "            )\n",
        "            pipe.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"sdxl_models\", weight_name=\"ip-adapter_sdxl.bin\")\n",
        "            pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config, use_karras_sigmas=True)\n",
        "            if self.device == \"cuda\":\n",
        "                pipe.enable_model_cpu_offload()\n",
        "            pipe.vae.enable_tiling()\n",
        "            self.scene_gen_pipe = pipe\n",
        "            print(\"‚úÖ SCENE BUILDER READY.\")\n",
        "        except Exception as e:\n",
        "            print(f\"<!> BUILDER LOAD ERROR: {e}\")\n",
        "\n",
        "    def load_realism_engine(self):\n",
        "        if self.i2v_pipe is not None: return\n",
        "        self._unload_all()\n",
        "        print(\"‚ö° LOADING ANIMATOR (SVD-XT)...\")\n",
        "        try:\n",
        "            repo_id = \"stabilityai/stable-video-diffusion-img2vid-xt\"\n",
        "            pipe = StableVideoDiffusionPipeline.from_pretrained(\n",
        "                repo_id, \n",
        "                torch_dtype=self.dtype, \n",
        "                variant=\"fp16\"\n",
        "            )\n",
        "            pipe.enable_model_cpu_offload()\n",
        "            if self.vram_gb < 10:\n",
        "                pipe.unet.enable_forward_chunking()\n",
        "            self.i2v_pipe = pipe\n",
        "            print(\"‚úÖ ANIMATOR READY (SVD-XT).\")\n",
        "        except Exception as e:\n",
        "            print(f\"<!> SVD LOAD ERROR: {e}\")\n",
        "\n",
        "    def load_action_engine(self):\n",
        "        if self.action_pipe is not None: return\n",
        "        self._unload_all()\n",
        "        print(\"‚ö° LOADING ACTION ENGINE (AnimateDiff Lightning)...\")\n",
        "        try:\n",
        "            ckpt_path = hf_hub_download(\"ByteDance/AnimateDiff-Lightning\", \"animatediff_lightning_4step_diffusers.safetensors\")\n",
        "            adapter = MotionAdapter.from_pretrained(\"guoyww/animatediff-motion-adapter-v1-5-2\", torch_dtype=self.dtype)\n",
        "            state_dict = load_file(ckpt_path, device=\"cpu\")\n",
        "            adapter.load_state_dict(state_dict)\n",
        "            pipe = AnimateDiffPipeline.from_pretrained(\n",
        "                \"emilianJR/epiCRealism\",\n",
        "                motion_adapter=adapter,\n",
        "                torch_dtype=self.dtype\n",
        "            )\n",
        "            pipe.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"models\", weight_name=\"ip-adapter_sd15.bin\")\n",
        "            pipe.scheduler = EulerDiscreteScheduler.from_config(\n",
        "                pipe.scheduler.config, timestep_spacing=\"trailing\", beta_schedule=\"linear\"\n",
        "            )\n",
        "            pipe.enable_model_cpu_offload()\n",
        "            pipe.vae.enable_tiling()\n",
        "            self.action_pipe = pipe\n",
        "            print(\"‚úÖ ACTION ENGINE READY.\")\n",
        "        except Exception as e:\n",
        "            print(f\"<!> ACTION ENGINE ERROR: {e}\")\n",
        "\n",
        "    def create_character(self, prompt):\n",
        "        print(f\"\\nüì∏ GENERATING NEW CHARACTER: '{prompt}'\")\n",
        "        self.load_scene_builder()\n",
        "        self.scene_gen_pipe.set_ip_adapter_scale(0.0)\n",
        "        full_prompt, negative = self._smart_prompt(prompt)\n",
        "        width, height = 832, 1216\n",
        "        dummy_img = Image.new(\"RGB\", (width, height), (0, 0, 0))\n",
        "        image = self.scene_gen_pipe(\n",
        "            full_prompt, \n",
        "            negative_prompt=negative, \n",
        "            ip_adapter_image=dummy_img,\n",
        "            height=height,\n",
        "            width=width,\n",
        "            num_inference_steps=6, \n",
        "            guidance_scale=2.0\n",
        "        ).images[0]\n",
        "        filename = f\"char_{int(time.time())}.png\"\n",
        "        image.save(filename)\n",
        "        self.current_character_path = os.path.abspath(filename)\n",
        "        return self.current_character_path, filename\n",
        "\n",
        "    def generate_realism(self, motion_val=5, fps=7):\n",
        "        if not self.current_character_path:\n",
        "            return None\n",
        "        self.load_realism_engine()\n",
        "        image = load_image(self.current_character_path)\n",
        "        image = image.resize((576, 1024))\n",
        "        \n",
        "        # Map 1-10 to SVD bucket\n",
        "        bucket = int(50 + (motion_val * 15)) # 1->65, 5->125, 10->200\n",
        "        \n",
        "        print(f\"üé¨ GENERATING REALISM CLIP (SVD-XT)...\")\n",
        "        frames = self.i2v_pipe(\n",
        "            image, \n",
        "            decode_chunk_size=2,\n",
        "            generator=torch.manual_seed(int(time.time())),\n",
        "            motion_bucket_id=bucket,\n",
        "            noise_aug_strength=0.1,\n",
        "            num_frames=25\n",
        "        ).frames[0]\n",
        "        filename = f\"realism_{os.urandom(4).hex()}.mp4\"\n",
        "        export_to_video(frames, output_video_path=filename, fps=fps)\n",
        "        return filename\n",
        "\n",
        "    def generate_action(self, prompt, duration_sec=4, motion_strength=5, upscale=False, progress_callback=None):\n",
        "        if not self.current_character_path: return None\n",
        "        self.load_action_engine()\n",
        "        full_prompt, neg = self._smart_prompt(prompt)\n",
        "        ref_image = load_image(self.current_character_path)\n",
        "        self.action_pipe.set_ip_adapter_scale(0.6)\n",
        "        neg = neg + \", static, motionless, frozen, stationary, picture, photo, glitch, morphing, disjointed\"\n",
        "        \n",
        "        if self.quality_mode == \"CINEMA\" or self.quality_mode == \"ULTRA\":\n",
        "            self.action_pipe.enable_free_init(method=\"butterworth\", num_iters=3)\n",
        "        else:\n",
        "            self.action_pipe.disable_free_init()\n",
        "\n",
        "        chunk_len, overlap = 16, 4\n",
        "        effective_len = chunk_len - overlap\n",
        "        target_frames = duration_sec * 8\n",
        "        num_chunks = int(target_frames / effective_len) + 1\n",
        "        all_frames = []\n",
        "\n",
        "        for i in range(num_chunks):\n",
        "            msg = f\"Rendering Clip {i+1}/{num_chunks}...\"\n",
        "            print(f\"   -> {msg}\")\n",
        "            if progress_callback: progress_callback(i, num_chunks, msg)\n",
        "            steps = 20 if self.quality_mode == \"ULTRA\" else (12 if self.quality_mode == \"CINEMA\" else 6)\n",
        "            output = self.action_pipe(\n",
        "                prompt=full_prompt,\n",
        "                ip_adapter_image=ref_image,\n",
        "                negative_prompt=neg,\n",
        "                num_frames=16,\n",
        "                guidance_scale=1.0 + (motion_strength * 0.1),\n",
        "                width=512, height=768,\n",
        "                num_inference_steps=steps,\n",
        "                generator=torch.manual_seed(42 + i*100)\n",
        "            )\n",
        "            new_frames = output.frames[0]\n",
        "            gc.collect()\n",
        "            if self.device == \"cuda\": torch.cuda.empty_cache()\n",
        "            \n",
        "            if i == 0:\n",
        "                all_frames.extend(new_frames)\n",
        "            else:\n",
        "                prev_segment = all_frames[-overlap:]\n",
        "                curr_segment = new_frames[:overlap]\n",
        "                blended_segment = []\n",
        "                for j in range(overlap):\n",
        "                    alpha = (j + 1) / (overlap + 1)\n",
        "                    frame_prev = Image.fromarray(np.array(prev_segment[j]))\n",
        "                    frame_curr = Image.fromarray(np.array(curr_segment[j]))\n",
        "                    blended = Image.blend(frame_prev, frame_curr, alpha)\n",
        "                    blended_segment.append(blended)\n",
        "                all_frames = all_frames[:-overlap] + blended_segment + new_frames[overlap:]\n",
        "\n",
        "        if upscale:\n",
        "            if progress_callback: progress_callback(num_chunks, num_chunks, \"Upscaling...\")\n",
        "            upscaled_frames = []\n",
        "            for frame in all_frames:\n",
        "                img = np.array(frame)\n",
        "                img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "                img = cv2.resize(img, (1024, 1536), interpolation=cv2.INTER_LANCZOS4)\n",
        "                kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])\n",
        "                img = cv2.filter2D(img, -1, kernel)\n",
        "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "                upscaled_frames.append(Image.fromarray(img))\n",
        "            all_frames = upscaled_frames\n",
        "\n",
        "        filename = f\"action_{os.urandom(4).hex()}_long.mp4\"\n",
        "        export_to_video(all_frames, output_video_path=filename, fps=8)\n",
        "        return filename\n",
        "\n",
        "# ==============================================================================\n",
        "# UI LAUNCHER\n",
        "# ==============================================================================\n",
        "\n",
        "director = TitanDirector()\n",
        "\n",
        "with gr.Blocks(theme=\"ocean\", title=\"TITAN CLOUD\") as app:\n",
        "    gr.Markdown(\"# ‚ö° TITAN CLOUD (Free GPU Edition)\")\n",
        "    \n",
        "    # STATES\n",
        "    current_char_img = gr.State(None)\n",
        "    \n",
        "    # STEP 1: IDENTITY\n",
        "    with gr.Group() as step1_group:\n",
        "        gr.Markdown(\"## 1Ô∏è‚É£ STEP 1: IDENTITY\")\n",
        "        with gr.Row():\n",
        "            preset_dd = gr.Dropdown(director.get_preset_list(), label=\"üíé Viral Style Preset\", value=\"None\")\n",
        "        with gr.Row():\n",
        "            char_prompt = gr.Textbox(label=\"Who are they?\", placeholder=\"A cyberpunk girl...\", lines=2)\n",
        "            gen_char_btn = gr.Button(\"CREATE CHARACTER\", variant=\"primary\")\n",
        "        char_output = gr.Image(label=\"Generated Character\", type=\"filepath\", height=400)\n",
        "        next_btn_1 = gr.Button(\"LOCK IDENTITY & NEXT ‚û°Ô∏è\", variant=\"secondary\")\n",
        "\n",
        "    # STEP 2: ACTION\n",
        "    with gr.Group(visible=False) as step2_group:\n",
        "        gr.Markdown(\"## 2Ô∏è‚É£ STEP 2: THE ACTION\")\n",
        "        engine_mode = gr.Radio(\n",
        "            [\"Action Director (AnimateDiff)\", \"Realism Engine (SVD-XT Pro)\"], \n",
        "            label=\"Engine\", value=\"Realism Engine (SVD-XT Pro)\"\n",
        "        )\n",
        "        action_prompt = gr.Textbox(label=\"Prompt\", placeholder=\"Dancing...\", lines=2)\n",
        "        with gr.Accordion(\"Settings\", open=False):\n",
        "            quality = gr.Radio([\"SPEED\", \"CINEMA\", \"ULTRA\"], label=\"Quality\", value=\"CINEMA\")\n",
        "            duration = gr.Slider(5, 30, 10, label=\"Duration (s)\")\n",
        "            motion = gr.Slider(1, 10, 5, label=\"Motion\")\n",
        "        \n",
        "        render_btn = gr.Button(\"GENERATE VIDEO üé¨\", variant=\"primary\")\n",
        "        video_output = gr.Video(label=\"Final Video\")\n",
        "        back_btn = gr.Button(\"‚¨ÖÔ∏è BACK\")\n",
        "\n",
        "    # EVENTS\n",
        "    def gen_char(p, preset):\n",
        "        full_p = director.apply_preset(p, preset)\n",
        "        path, name = director.create_character(full_p)\n",
        "        return path\n",
        "    gen_char_btn.click(gen_char, inputs=[char_prompt, preset_dd], outputs=char_output)\n",
        "    \n",
        "    def lock_and_next(img):\n",
        "        if not img: return {step1_group: gr.update(visible=True)}\n",
        "        director.set_character(img)\n",
        "        return {step1_group: gr.update(visible=False), step2_group: gr.update(visible=True)}\n",
        "    next_btn_1.click(lock_and_next, inputs=char_output, outputs=[step1_group, step2_group])\n",
        "    \n",
        "    def go_back():\n",
        "        return {step1_group: gr.update(visible=True), step2_group: gr.update(visible=False)}\n",
        "    back_btn.click(go_back, outputs=[step1_group, step2_group])\n",
        "\n",
        "    def render(eng, p, q, d, m, progress=gr.Progress()):\n",
        "        director.set_quality(q)\n",
        "        if \"SVD\" in eng:\n",
        "            progress(0, desc=\"Generating Realism Clip...\")\n",
        "            return director.generate_realism(motion_val=m)\n",
        "        else:\n",
        "            def cb(i, t, msg): progress(i/t, desc=msg)\n",
        "            return director.generate_action(p, d, m, upscale=True, progress_callback=cb)\n",
        "            \n",
        "    render_btn.click(render, inputs=[engine_mode, action_prompt, quality, duration, motion], outputs=video_output)\n",
        "\n",
        "app.queue().launch(share=True, debug=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}